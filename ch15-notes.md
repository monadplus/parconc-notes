# Chapter 15. Debugging, Tuning, and Interfacing with Foreign Code

## Debugging Concurrent Programs

Tricks and techniques that you might find useful when debugging concurrency.

### Inspecting the Status of a Thread

The `threadStatus` function (from _GHC.Conc_):

```haskell
threadStatus :: ThreadId -> IO ThreadStatus

data ThreadStatus
 = ThreadRunning
 | ThreadFinished
 | ThreadBlocked BlockReason
 | ThreadDied -- This should never happen under normal circumstances
              -- `forkIO` includes a default exception handle that catches and prints exceptions.
 deriving (Eq, Ord, Show)

data BlockReason
  = BlockedOnMVar
  | BlockedOnBlackHole
  | BlockedOnException
  | BlockedOnSTM
  | BlockedOnForeignCall
  | BlockedOnOther
  deriving (Eq, Ord, Show)
```

Here's an example in GHCi:

```
>>> t <- forkIO (threadDelay 3000000)
>>> GHC.Conc.threadStatus t
ThreadBlocked BlockedOnMVar
>>> -- wait a few seconds
>>> GHC.Conc.threadStatus t
ThreadFinished
```

> Don't use it for normal flow control in your program: breaks abstraction (previous example) and the status is outdated as soon as threadStatus returns.

### Event Logging and ThreadScope

`putStrLn` is useful but not enough lightweight. It can introduce contention for the `stdout Handle`, which may perturb the concurrency in the program you're trying to debug.

We've used _ThreadScope_ a lot for diagnose performance problems in this book. _ThreadScope_ generates its graphs from the information in the _.eventlog_ file that is produced when we run a program with the `+RTS -l` option.

You may have noticed that ThreadScope identifies threads by their number. For debugging, it helps a lot to know which thread in the program corresponds to which thread number; this connection can be made using: `labelThread`:

```haskell
labelThread :: ThreadId -> String -> IO () -- defined in GHC.Conc
```

`labelThread` emit a special event into the _eventlog_ file.

There are also a couple of ways to put information in the _eventlog_ file:

```haskell
traceEvent   :: String -> a -> a
traveEventIO :: String -> IO ()  -- defined in Debug.Trace
```

Here is a simple program using `labelThread` and `traceEventIO`: _mvar4.hs_

```haskell
main = do
  t <- myThreadId
  labelThread t "main"
  m <- newEmptyMVar
  t <- forkIO $ putMVar m 'a'
  labelThread t "a"
  t <- forkIO $ putMVar m 'b'
  labelThread t "b"
  traceEventIO "before takeMVar"
  takeMVar m
  takeMVar m
```

Compile the program with `-eventlog` and run it with `+RTS -l`:

```zsh
$ ghc mvar4.hs -threaded -eventlog
$ ./mvar4 +RTS -l # Generates mvar4.eventlog

# Alternative: stack exec mvar4 +RTS -l
# For some reasons it doesn't work, only if you manually execute the binary from /Users/arnau/haskell/parconc-examples/.stack-work/install/x86_64-osx/lts-8.21/8.0.2/bin/
```

You can use both `ThreadScope` and `ghc-events` to display it:

`$ ghc-events show mvar4.eventlog`

We labeled the main thread "main" so search for it on the logs:

```
7183316: cap 0: running thread 4
7202882: cap 0: thread 4 has label "main"                -- This event was generated by labelThread
7203445: cap 0: creating thread 5                        -- First forIO
7204394: cap 0: stopping thread 4 (thread yielding)
7209277: cap 0: running thread 5                         -- putMVar
7211956: cap 0: stopping thread 5 (thread finished)      -- dies
7215458: cap 0: running thread 4
7216757: cap 0: thread 5 has label "a"                   -- the label is set after it dies...
7217246: cap 0: creating thread 6                        -- second forkIO
7218241: cap 0: thread 6 has label "b"
7221303: cap 0: before takeMVar                          -- traceEventIO
7221798: cap 0: stopping thread 4 (blocked on an MVar)
7224665: cap 0: running thread 6
7227290: cap 0: waking up thread 4 on cap 0
7227522: cap 0: stopping thread 6 (thread finished)      -- 6 can't finished before main thread takes value from MVar
7230969: cap 0: running thread 4
7231132: cap 0: stopping thread 4 (thread finished)
```

### Detecting Deadlock

GHC runtime system can detect when a thread has become deadlocked and send it the `BlockedIndefinitelyOnMVar` exception.

How it works?

In GHC both threads and MVars are objects on the heap, just like other data values. An MVar that has blocked threads is represented by a heap object that points to a list of the blocked threads. Heap objects are managed by the garbage collector, which traverses the heap starting from the roots to discover all the live objects. The set of roots consists of the running threads and the stack associated with each of these threads. Any thread that is not reachable from the roots is definitely deadlocked. The runtime system cannot ever find these threads by following pointers, so they can never become runnable again.

For example, if a thread is blocked in takeMVar on an MVar that is not referenced by any other thread, then both the MVar that it is blocked on and the thread itself will be unreachable. When a thread is found to be unreachable, it is sent the `BlockedIndefinitelyOnMVar` exception (there is also a `BlockedIndefinitelyOnSTM` exception for when a thread is blocked in an STM transaction). The exception gives the thread a chance to clean up any resources it may have been holding and also allows the program to quit with an error message rather than hanging in the event of a deadlock.

The concept extends to mutual deadlock between a group of threads. Suppose we create two threads that deadlock on each other like this:

```haskell
a <- newEmptyMVar
b <- newEmptyMVar
forkIO (do takeMVar a; putMVar b ())
forkIO (do takeMVar b; putMVar a ())
```

Both threads are blocked, each on a MVar that is reachable from the other. As far as the garbage collector is concerned, both threads and the MVars a anb b are unreachable.

Some consequences that might not be immediately obvious _deadlock1.hs_:

```haskell
main = do
  lock <- newEmptyMVar
  complete <- newEmptyMVar
  forkIO $ takeMVar lock `finally` putMVar complete ()
  takeMVar complete
```

The child thread is clearly deadlocked, and so it should receive the `BlockedIndefinitelyOnMVar` exception. This will cause the `finally` action and unblock the main thread.

However, that is not what happens. At the point where the child thread is deadlocked, _the main thread is also deadlocked_.

The second consequence is taht the runtime can't always prove taht a thread is deadlocked _deadlock2.hs_:

```haskell
main = do
  lock <- newEmptyMVar
  forkIO $ do r <- try (takeMVar lock); print (r :: Either SomeException ())
  threadDelay 1000000
  print (lock == lock)
```

The child thread nevers receives an exception, and the program completes printing True.
The reason the deadlock is not detected here is taht the main thread is holding a reference to the MVar lock `(lock == lock)`.

> You can't in general know how clever the compiler is going to be, so you should not rely on deadlock detection for the correct working of your program.

## Tuning Concurrent (and Parallel) Programs

- Avoid premature optimization.
- Don't waste time optimizing code that doesn't contribute much to over-all runtime.
	- GHC has a reasonable space and time profile (next section).
	- In concurrent programs, the problem can often be I/O or contention, in which case use _ThreadScope_ and `labelThread` and `traceEvent`.

### Profiling Time Usage

_Main.hs_

```haskell
main = print (f 30 + g 30)
  where
    f n  = fib n
    g n  = fib (n `div` 2)

fib n = if n < 2 then 1 else fib (n-1) + fib (n-2)
```

Compile with `-prof` and `fprof-auto` and run it with `+RTS -p`:

```
$ ghc -prof -fprof-auto -rtsopts Main.hs
$ ./Main +RTS -p
121393
```

Take a look at _Main.prof_ generated file:

```
	Tue Oct  8 08:22 2019 Time and Allocation Profiling Report  (Final)

	   Main +RTS -p -RTS

	total time  =        0.27 secs   (274 ticks @ 1000 us, 1 processor)
	total alloc = 538,951,480 bytes  (excludes profiling overheads)

COST CENTRE MODULE    SRC             %time %alloc

fib         Main      Main.hs:6:1-50  100.0  100.0


                                                                           individual      inherited
COST CENTRE  MODULE                SRC                  no.     entries  %time %alloc   %time %alloc

MAIN         MAIN                  <built-in>            46          0    0.0    0.0   100.0  100.0
 CAF         Main                  <entire-module>       91          0    0.0    0.0   100.0  100.0
  main       Main                  Main.hs:(1,1)-(4,26)  92          1    0.0    0.0   100.0  100.0
   main.f    Main                  Main.hs:3:5-16        95          1    0.0    0.0    99.3   99.9
    fib      Main                  Main.hs:6:1-50        96    2692537   99.3   99.9    99.3   99.9
   main.g    Main                  Main.hs:4:5-26        93          1    0.0    0.0     0.7    0.1
    fib      Main                  Main.hs:6:1-50        94       1973    0.7    0.1     0.7    0.1
 CAF         GHC.Conc.Signal       <entire-module>       87          0    0.0    0.0     0.0    0.0
 CAF         GHC.IO.Encoding       <entire-module>       81          0    0.0    0.0     0.0    0.0
 CAF         GHC.IO.Encoding.Iconv <entire-module>       79          0    0.0    0.0     0.0    0.0
 CAF         GHC.IO.Handle.FD      <entire-module>       71          0    0.0    0.0     0.0    0.0
 CAF         GHC.IO.Handle.Text    <entire-module>       69          0    0.0    0.0     0.0    0.0
```

### Profiling Memory Usage (heap)

https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html#profiling-memory-usage

### Thread Creation and MVar Operations

GHC strivees to provide an extremely efficient implementation of threads.

Let's explore the performance of a couple of very simple concurrent programs:

_threadperf1.hs_

```haskell
numThreads = 1000000

main = do
	m <- newEmptyMVar
	replicateM_ numThreads $ forkIO (putMVar m ())
	replicateM_ numThreads $ takeMVar m
```

Memory overhead for threads because all the threads will be resident in memory at once.

```zsh
# $ ./threadperf1 +RTS -s
$ /Users/arnau/haskell/parconc-examples/.stack-work/install/x86_64-osx/lts-8.21/8.0.2/bin/threadperf1 +RTS -s

   1,064,040,992 bytes allocated in the heap
   4,294,827,688 bytes copied during GC
     961,913,720 bytes maximum residency (13 sample(s))
     137,182,816 bytes maximum slop
            2106 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      2033 colls,     0 par    0.472s   0.485s     0.0002s    0.1718s
  Gen  1        13 colls,     0 par    1.298s   1.990s     0.1531s    0.7159s

  INIT    time    0.000s  (  0.003s elapsed)
  MUT     time    0.000s  ( -0.206s elapsed)
  GC      time    1.770s  (  2.475s elapsed)
  EXIT    time    0.422s  (  0.585s elapsed)
  Total   time    2.141s  (  2.857s elapsed)

  %GC     time      82.7%  (86.6% elapsed)

  Alloc rate    0 bytes per MUT second

  Productivity  17.3% of total user, 13.3% of total elapsed
```

So about 1 GB was allocated, although the total memory required by the program was 2.1 GB (2106MB).

The amount of allocated memory tells us that threads required approx 1 KB each, and the extra memory used by the program is due to copying GC overhead.

In fact, it is possible to tune the amount of memory given to a thread: `+RTS -k<size>` bytes:

```zsh
$  ghc -rtsopts -threaded threadperf1.hs
$ ./threadperf1 +RTS -s -k400

     600,105,832 bytes allocated in the heap
   1,410,160,400 bytes copied during GC
     291,245,192 bytes maximum residency (11 sample(s))
      65,922,480 bytes maximum slop
             661 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0      1163 colls,     0 par    0.327s   0.338s     0.0003s    0.0079s
  Gen  1        11 colls,     0 par    0.762s   1.029s     0.0935s    0.3302s

  INIT    time    0.000s  (  0.002s elapsed)
  MUT     time    0.001s  ( -0.069s elapsed)
  GC      time    1.089s  (  1.367s elapsed)
  EXIT    time    0.316s  (  0.398s elapsed)
  Total   time    1.407s  (  1.698s elapsed)

  %GC     time      77.4%  (80.5% elapsed)

  Alloc rate    1,013,692,283,783 bytes per MUT second

  Productivity  22.6% of total user, 19.4% of total elapsed
```

A thread will allocate more memory for its stack on demand, so whether it is actualy a good idea to use `+RTS -k400` will depend on your program.

In this case, the threas were doing very little so it helped the overall performance.

_threadperf2.hs_:

```haskell
numThreads = 1000000

main = do
  ms <- replicateM numThreads $ do
          m <- newEmptyMVar
          forkIO (putMVar m ())
          return m
  mapM_ takeMVar ms
```

This program has quite different performance characteristics:

```zsh
$ ./threadperf2 +RTS -s

   1,153,017,744 bytes allocated in the heap
     267,061,032 bytes copied during GC
      62,962,152 bytes maximum residency (8 sample(s))
       4,662,808 bytes maximum slop
             121 MB total memory in use (0 MB lost due to fragmentation)

  INIT    time    0.00s  (  0.00s elapsed)
  MUT     time    0.70s  (  0.72s elapsed)
  GC      time    0.50s  (  0.50s elapsed)
  EXIT    time    0.02s  (  0.02s elapsed)
  Total   time    1.22s  (  1.24s elapsed)
```

The total memory in use by the program at any one tiem was only 121 MB.

This is because each thread can run to completition indendently.

Note that the GC overhead of this program are much lower than the first example.

The total time gives us a rough indication of the time it takes to create an MVar, and a thread, and for the thread to run, put into the MVar, complete, and be garbage-collected. We did this 1.000.000 times in about 1.2 s, so the time per thread is about 1.2 microseconds.


### Shared Concurrent Data Structures

Brief summarize of shared state options, with a focus on the performance implications of the different choices.

Typically, the best approach when you want some shared state is to take an existing pure data structure, such as a list or a Map, and store it in a mutable container.

There are a couple of subtle performance issues to be aware of, though.

The first is the effect of lazy evaluation when writing a new value int othe container, which we've already covered.

The second is the choice of mutable contaienr itself, which exposes some subtle performance trade-offs. There are three choices:

- MVar: we found in Limiting the Number of Threads with a Semaphore that using an MVar to keep a shared counter did not perform well under high contention. This is a consequence of the fairness guarantee that MVar offers.

- TVar: sometimes performs better than MVar under contention and has the advantage of being composable with other STM operations. However, be aware of the other performance pitfalls with STM described in Performance.

- IORef: using an IORef together with `atomicModifyIORef` is often a good choice for performance. The main pitfall here is lazy evaluation; getting enough strictness when using `atomicModifyIORef` is quite tricky. This is a good pattern to follow:

```haskell
b <- atomicModifyIORef ref
        (\x -> let (a, b) = f x
               in (a, a `seq` b))
b `seq` return b
```

The `seq` call on the last line forces the second component of the pair, which itself is a `seq` call that forces `a`, which in turn forces the call to `f`.

### RTS Options to Tweak

GHC has plenty of options to tune the RTS. Here, I'll highlight a few of the options that are good targets for tuning concurrent and parallel programs.

The RTS should be placed after `+RST`:

`-N[cores]` (default 1): GHC can automatically determine the number of processors in your machine if you use `-N`, but that might not always be the best choice. The GHC runtime system scales well when it has exclusive access to the number of processors specified with -N, but performance can degrade quite rapidly if there is contention.

`-qa` (default off): enables the use of _processors affinity_, which locks the haskell program to specific cores. Normally the operating system is free to migrate the threads that run the Haskell program around the cores in the machine in response to other activity, but using `-qa` prevents it from doing so. This can improve performance or degradate, depending on the scheduling.

`-Asize` (default 512k): This option controls the size of the memory allocation are for each core. A good rule of thumb is to keep this around the size of the L@ cache per core on your machine. Cache sizes vary a lot and are often shared between cores. So setting the `-A` value is not an exact science. There are two opposing factors to play here: using more memory means we run the garbage collector less, but using less memory means we use the caches mores. The sweet spot depends on the characteristics of the program and the hardware.

`Iseconds` (default 0.3): this options affects deadlock detections. The runtime needs to perform a full garbage collection in order to detect deadlocked threads. When the program is idle, the runtime doesn't know whether a thread will wake up again, or the program is deadlocked and the garbage collector shoudl be run to detect the deadlock. Waits n (default 0.3) seconds before running the garbage collector. You might want to turn this value higher.

`-C[seconds]` (default 0.02): context-switching interval, which determines how often the scheduler interrupts the current thread to run the next thread on the run queue. As a rule of thumb, this option should not be set too low because frequent context switches harm performance, and should not be set too high because that can cause jerkiness and stutering in interactive threads.

## Concurrency and the Foreign Function Interface (FFI)

Foreign langauges also have their own threading models - in C, there are POSIX and Win32 threads, for example - so we need to specify how Concurrent Haskell interacts with the threading models of foreign code.

All of the following assumes the use of GHC's `-threaded` option. Without `-threaded`, the Haskell process uses a single OS thread only, and multithreaded foreign calls are not supported.

### Threads and Foreign Out-Calls

An _out-call_ is a call made from Haskell to a foreign language. At the present time, the FFI supports only calls to C, so that's all we describe here.

In the following, we refer to threads in C (i.e. POSIX or Win32 threads) as "OS threads" to distinguish them from the Haskell threads created with `forkIO`.

As an example:

```haskell
foreign import ccall "read"
  c_read :: CInt       -- ^ file descriptor
         -> Ptr Word8  -- ^ buffer for data
         -> CSize      -- ^ size of buffer
         -> CSSize     -- ^ bytes reads, or -1 on error
```

This declares a haskell function `c_read` that can be used to call the C function `read()`.

When a haskell thread makes a foreign call, that foreign call runs concurrently with the other Haskell threads, and indeed with any other active foreign calls. The only way that two C calls can be running concurrently is if they are running in two separate OS threads, so that is exactly what happens; if several Haskell threads call `c_read` and they all block waiting for data to be read, there will be one OS thread per call blocked in `read()`.

To handle concurrent foreign calls, the runtime system has to create more OS threads, and in fact it does this on demant.

The implicication of this design is that foreign call may be executed in _any_ OS thread, and subsequent calls may even be executed in different OS threads. In most cases, this isn't a problem, but sometimes it is; some foreign code must be called by a _particular_ OS thread:

- Libraries that allow only one OS thread to use their API. _GUI_ libraries often fall into this category. It must often be called by the main thread.
- APIs that use internal thread-local state. The best known example of this is _OpenGL_, which supports multithreaded use but stored state between API calls in thread-local storage. Hence, subsequent call must be made in the same OS thread; otherwise, the later call will see the wrong state.

To handle these requirement, Haskell has a concept of __bound threads__. A bound thread is a Haskell thread/OS thread pair that guarantees that foreign calls made by the Haskell thread always take palce in the associated OS thread. A bounded thread is created by `forkOS`:

```haskell
forkOS :: IO () -> IO ThreadId
```

Care should be taken when calling `forkOS`; it creates a complete new OS thread, so it can be quite expensive. Furthermore, bound threads are much more expensive that unbound threads.

For more details [Control.Concurrent](https://hackage.haskell.org/package/base-4.12.0.0/docs/Control-Concurrent.html#g:8).

> The thread that runs `main` in a Haskell program is a bound thread (slow). The best way around this problem is just to create a new thread from the main and work in that instead.

### Asynchronous Exceptions and Foreign Calls

When a Haskell thread is making a foreign call, it cannot receive asynchronous exceptions. There is no way in general to interrupt a foreign call. This means that a thread blocked in a foreign call may be unresponsive to timeouts and interrupts, and moreover that calling `throwTo` will block if the target thread is in a foreign call.

The trick for working around this limitation is to perform the foreign call in a separate thread:

```haskell
do
  a <- async $ c_read fb buf size
  r <- wait a
```

Now, the current thread is blocked in `wait` and can be interrupted by an exception as usual. Note that if an exception is raised it won't cancel the read() call. `withAsync` doesn't work and blocks the current thread.

### Threads and Foreign In-Calls

_In-calls_ are calls to Haskell functions that have been exposed to foreign code with a _foreign export_ declaration. For example, if we have a function f of type Int -> IO Int, we could expose it like this:

```haskell
foreign export ccall "f" f :: Int -> IO Int
```

This would create a C function with the following signature:

```c
HsInt f(HsInt0;
```

In a multithreaded program, it is entirely possible for `f` to be called by multiple OS threads concurrently. The GHC runtime system supports this (provided you use `-threaded)` with the following behavior: each call becomes a new bound thread. That is, a new Haskell thread is created for each call, and the Haskell thread is bound to the OS thread that made the call. Hence, any further out-calls made by the Haskell thread will take place in the same OS thread that made the original in-call. This turns out to be important for dealing with GUI callbacks. The GUI wants to run in the main OS thread only, so when it makes a callback into Haskell, we need to ensure that GUI calls made by the callback happen in the same OS thread that invoked the callback.

